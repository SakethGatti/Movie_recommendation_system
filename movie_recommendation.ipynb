{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c918179",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "print ('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188123d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"ml-100k\"):\n",
    "    # # Download the dataset\n",
    "    !wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "    # # Unzip it\n",
    "    !unzip ml-100k.zip\n",
    "    # # Get rid of the .zip file\n",
    "    !rm ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620eb83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\n",
    "    \"ml-100k/u.data\",\n",
    "    sep=\"\\t\", # this is a tab separated data\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"], # the columns names\n",
    "    usecols=[\"user_id\", \"movie_id\", \"rating\"], # we do not need the timestamp column\n",
    "    low_memory=False\n",
    ")\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233c626",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_perc = 0.2\n",
    "\n",
    "# Initialize the train and test dataframes.\n",
    "train_set, test_set = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Check each user.\n",
    "for user_id in ratings.user_id.unique():\n",
    "    user_df = ratings[ratings.user_id == user_id].sample(\n",
    "        frac=1,\n",
    "        random_state=42\n",
    "    ) # select only samples of the actual user and shuffle the resulting dataframe\n",
    "    \n",
    "    n_entries = len(user_df)\n",
    "    n_test = int(round(test_perc * n_entries))\n",
    "    \n",
    "    test_set = pd.concat((test_set, user_df.tail(n_test)))\n",
    "    train_set = pd.concat((train_set, user_df.head(n_entries - n_test)))\n",
    "\n",
    "train_set = train_set.sample(frac=1).reset_index(drop=True)\n",
    "test_set = test_set.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc811712",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_interactions_matrix(r_mat, n_users, n_items):\n",
    "    iter_m = np.zeros((n_users, n_items))\n",
    "    \n",
    "    for _, user_id, movie_id, rating in r_mat.itertuples():\n",
    "        iter_m[user_id-1, movie_id-1] = rating\n",
    "    \n",
    "    return iter_m\n",
    "\n",
    "iter_m = build_interactions_matrix(ratings, n_users, n_movies)\n",
    "iter_m.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e86567",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_similarity_matrix(interactions_matrix, kind=\"user\", eps=1e-9):\n",
    "    # takes rows as user features\n",
    "    if kind == \"user\":\n",
    "        similarity_matrix = interactions_matrix.dot(interactions_matrix.T)\n",
    "    # takes columns as item features\n",
    "    elif kind == \"item\":\n",
    "        similarity_matrix = interactions_matrix.T.dot(interactions_matrix)\n",
    "    norms = np.sqrt(similarity_matrix.diagonal()) + eps\n",
    "    return similarity_matrix / (norms[np.newaxis, :] * norms[:, np.newaxis])\n",
    "\n",
    "u_sim = build_similarity_matrix(iter_m, kind=\"user\")\n",
    "i_sim = build_similarity_matrix(iter_m, kind=\"item\")\n",
    "\n",
    "print(f\"User similarity matrix shape: {u_sim.shape}\\nUser similarity matrix sample:\\n{u_sim[:4, :4]}\")\n",
    "print(\"-\" * 97)\n",
    "print(f\"Item similarity matrix shape: {i_sim.shape}\\nItem similarity matrix sample:\\n{i_sim[:4, :4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f59564",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_users,\n",
    "        n_items,\n",
    "        r_mat,\n",
    "        kind=\"user\",\n",
    "        eps=1e-9,\n",
    "    ):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.kind = kind\n",
    "        self.eps = eps\n",
    "        self.iter_m = build_interactions_matrix(r_mat, self.n_users, self.n_items)\n",
    "        self.sim_m = build_similarity_matrix(self.iter_m, kind=self.kind)\n",
    "        self.predictions = self._predict_all()\n",
    "    \n",
    "    def _predict_all(self):\n",
    "        if self.kind == \"user\":\n",
    "            predictions = \\\n",
    "                self.sim_m.dot(self.iter_m) / np.abs(self.sim_m + self.eps).sum(axis=0)[:, np.newaxis]\n",
    "        elif self.kind == \"item\":\n",
    "            predictions = \\\n",
    "                self.iter_m.dot(self.sim_m) / np.abs(self.sim_m + self.eps).sum(axis=0)[np.newaxis, :]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b00b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"User-based predictions sample:\")\n",
    "print(Recommender(n_users, n_movies, train_set, kind=\"user\").predictions[:4, :4])\n",
    "print(\"-\" * 97)\n",
    "print(\"item-based predictions sample:\")\n",
    "print(Recommender(n_users, n_movies, train_set, kind=\"item\").predictions[:4, :4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d8c2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_predictions_df(preds_m, dataframe):\n",
    "    preds_v = []\n",
    "    for row_id, user_id, movie_id, _ in dataframe.itertuples():\n",
    "        preds_v.append(preds_m[user_id-1, movie_id-1])\n",
    "    preds_df = pd.DataFrame(data={\"user_id\": dataframe.user_id, \"movie_id\": dataframe.movie_id, \"rating\": preds_v})\n",
    "    return preds_df\n",
    "\n",
    "def get_mse(estimator, train_set, test_set):\n",
    "    train_preds = build_predictions_df(estimator.predictions, train_set)\n",
    "    test_preds = build_predictions_df(estimator.predictions, test_set)\n",
    "    \n",
    "    train_mse = mean_squared_error(train_set.rating, train_preds.rating)\n",
    "    test_mse = mean_squared_error(test_set.rating, test_preds.rating)\n",
    "    \n",
    "    return train_mse, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed494c43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_mse, test_mse = get_mse(\n",
    "    Recommender(n_users, n_movies, train_set, kind=\"user\"),\n",
    "    train_set,\n",
    "    test_set\n",
    ")\n",
    "\n",
    "print(f\"User-based train MSE: {train_mse} -- User-based test MSE: {test_mse}\")\n",
    "print(\"-\" * 97)\n",
    "\n",
    "train_mse, test_mse = get_mse(\n",
    "    Recommender(n_users, n_movies, train_set, kind=\"item\"),\n",
    "    train_set,\n",
    "    test_set\n",
    ")\n",
    "\n",
    "print(f\"Item-based train MSE: {train_mse} -- Item-based test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b5da2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_users,\n",
    "        n_items,\n",
    "        r_mat,\n",
    "        k=40, # the number of neighbors to use when computing the similarity score\n",
    "        kind=\"user\",\n",
    "        eps=1e-9\n",
    "    ):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.kind = kind\n",
    "        self.eps = eps\n",
    "        self.iter_m = build_interactions_matrix(r_mat, self.n_users, self.n_items)\n",
    "        self.sim_m = build_similarity_matrix(self.iter_m, kind=self.kind)\n",
    "        self.k = k\n",
    "        self.predictions = self._predict_all()\n",
    "    \n",
    "    def _predict_all(self):\n",
    "        pred = np.empty_like(self.iter_m)\n",
    "        if self.kind == \"user\":\n",
    "            # An user has the higher similarity score with itself,\n",
    "            # so we skip the first element.\n",
    "            sorted_ids = np.argsort(-self.sim_m)[:, 1:self.k+1]\n",
    "            for user_id, k_users in enumerate(sorted_ids):\n",
    "                pred[user_id, :] = self.sim_m[user_id, k_users].dot(self.iter_m[k_users, :])\n",
    "                pred[user_id, :] /= np.abs(self.sim_m[user_id, k_users] + self.eps).sum()\n",
    "        elif self.kind == \"item\":\n",
    "            # An item has the higher similarity score with itself,\n",
    "            # so we skip the first element.\n",
    "            sorted_ids = np.argsort(-self.sim_m)[:, 1:self.k+1]\n",
    "            for item_id, k_items in enumerate(sorted_ids):\n",
    "                pred[:, item_id] = self.sim_m[item_id, k_items].dot(self.iter_m[:, k_items].T)\n",
    "                pred[:, item_id] /= np.abs(self.sim_m[item_id, k_items] + self.eps).sum()\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aeb7cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_mse, test_mse = get_mse(\n",
    "    Recommender(n_users, n_movies, train_set, kind=\"user\"),\n",
    "    train_set,\n",
    "    test_set\n",
    ")\n",
    "\n",
    "print(f\"User-based train MSE: {train_mse} -- User-based test MSE: {test_mse}\")\n",
    "print(\"-\" * 97)\n",
    "\n",
    "train_mse, test_mse = get_mse(\n",
    "    Recommender(n_users, n_movies, train_set, kind=\"item\"),\n",
    "    train_set,\n",
    "    test_set\n",
    ")\n",
    "\n",
    "print(f\"Item-based train MSE: {train_mse} -- Item-based test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077816d",
   "metadata": {},
   "source": [
    "Tuning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121320bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # The list of hyper-parameters we want to optmizer. For each one we define the bounds\n",
    "    # and the corresponding name.\n",
    "    k = trial.suggest_int(\"k\", 10, 200)\n",
    "    bias_sub = trial.suggest_categorical(\"bias_sub\", [False, True])\n",
    "\n",
    "    # Instantiating the model\n",
    "    model = Recommender(n_users, n_movies, train_set, kind=\"item\", k=k, bias_sub=bias_sub)\n",
    "    # Evaluating the performance\n",
    "    _, test_mse = get_mse(model, train_set, test_set)\n",
    "    return test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd0c01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "# Here the parameter search effectively begins.\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617a036",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_k = study.best_params[\"k\"]\n",
    "best_bias_sub = study.best_params[\"bias_sub\"]\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(f\"  - k = {best_k}\")\n",
    "print(f\"  - bias_sub = {best_bias_sub}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f92587",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_optimization_history(study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fedc86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_users,\n",
    "        n_items,\n",
    "        r_mat,\n",
    "        k=40,\n",
    "        kind=\"user\",\n",
    "        bias_sub=False,\n",
    "        eps=1e-9\n",
    "    ):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.kind = kind\n",
    "        self.iter_m = build_interactions_matrix(r_mat, self.n_users, self.n_items)\n",
    "        self.sim_m = build_similarity_matrix(self.iter_m, kind=self.kind)\n",
    "        self.bias_sub = bias_sub\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.predictions = self._predict_all()\n",
    "    \n",
    "    def _predict_all(self):\n",
    "        pred = np.empty_like(self.iter_m)\n",
    "        if self.kind == \"user\":\n",
    "            # Computes the new interaction matrix if needed.\n",
    "            iter_m = self.iter_m\n",
    "            if self.bias_sub:\n",
    "                user_bias = self.iter_m.mean(axis=1)[:, np.newaxis]\n",
    "                iter_m -= user_bias\n",
    "            # An user has the higher similarity score with itself,\n",
    "            # so we skip the first element.\n",
    "            sorted_ids = np.argsort(-self.sim_m)[:, 1:self.k+1]\n",
    "            for user_id, k_users in enumerate(sorted_ids):\n",
    "                pred[user_id, :] = self.sim_m[user_id, k_users].dot(iter_m[k_users, :])\n",
    "                pred[user_id, :] /= np.abs(self.sim_m[user_id, k_users]).sum() + self.eps\n",
    "            if self.bias_sub:\n",
    "                pred += user_bias\n",
    "            \n",
    "        elif self.kind == \"item\":\n",
    "            # Computes the new interaction matrix if needed.\n",
    "            iter_m = self.iter_m\n",
    "            if self.bias_sub:\n",
    "                item_bias = self.iter_m.mean(axis=0)[np.newaxis, :]\n",
    "                iter_m -= item_bias\n",
    "            # An item has the higher similarity score with itself,\n",
    "            # so we skip the first element.\n",
    "            sorted_ids = np.argsort(-self.sim_m)[:, 1:self.k+1]\n",
    "            for item_id, k_items in enumerate(sorted_ids):\n",
    "                pred[:, item_id] = self.sim_m[item_id, k_items].dot(iter_m[:, k_items].T)\n",
    "                pred[:, item_id] /= np.abs(self.sim_m[item_id, k_items]).sum() + self.eps\n",
    "            if self.bias_sub:\n",
    "                pred += item_bias\n",
    "                \n",
    "        return pred.clip(0, 5)\n",
    "    \n",
    "    def get_top_recomendations(self, item_id, n=6):\n",
    "        if self.kind == \"user\":\n",
    "            # For an user-based system, only similarities between users were computed.\n",
    "            # This strategy will not be covered in this post, but a solution to this\n",
    "            # could be of finding the top better rated items of similiar users.\n",
    "            # I'll leave this exercise to you =]\n",
    "            pass\n",
    "        if self.kind == \"item\":\n",
    "            sim_row = self.sim_m[item_id - 1, :]\n",
    "            # once again, we skip the first item for obviouos reasons.\n",
    "            items_idxs = np.argsort(-sim_row)[1:n+1]\n",
    "            similarities = sim_row[items_idxs]\n",
    "            return items_idxs + 1, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28521fde",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rs_model = Recommender(\n",
    "    n_users, \n",
    "    n_movies, \n",
    "    ratings, # the model will be built on the full dataset now\n",
    "    k=best_k, \n",
    "    kind=\"item\", \n",
    "    bias_sub=best_bias_sub\n",
    ")\n",
    "get_mse(rs_model, train_set, test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84abb6d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def title2id(mapper_df, movie_title):\n",
    "    return mapper_df.loc[mapper_df.movie_title == movie_title, \"movie_title\"].index.values[0]\n",
    "\n",
    "def ids2title(mapper_df, ids_list):\n",
    "    titles = []\n",
    "    for id in ids_list:\n",
    "        titles.append(mapper_df.loc[id, \"movie_title\"])\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1869004",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Columns names\n",
    "movies_mapper_cols = [\n",
    "    \"movie_id\", \n",
    "    \"movie_title\", \n",
    "    \"release_date\", \n",
    "    \"video_release_date\", \n",
    "    \"IMDb_URL\", \n",
    "    \"unknown\",\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Childrens\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film_Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci_Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\" \n",
    "]\n",
    "movies_mapper = pd.read_csv(\n",
    "    \"ml-100k/u.item\",\n",
    "    sep=\"|\",\n",
    "    encoding=\"latin\",\n",
    "    names=movies_mapper_cols,\n",
    "    usecols=[\"movie_id\", \"movie_title\"], # we only need these columns\n",
    "    index_col=\"movie_id\"\n",
    ")\n",
    "# Remove movies release years from titles\n",
    "movies_mapper[\"movie_title\"] = movies_mapper[\"movie_title\"].apply(\n",
    "    lambda title: re.sub(\"\\(\\d{4}\\)\", \"\", title).strip()\n",
    ")\n",
    "movies_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e370cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def print_recommendations(model, mapper, movie_title):\n",
    "    ids_list, similarities = rs_model.get_top_recomendations(title2id(mapper, movie_title))\n",
    "    titles = ids2title(movies_mapper, ids_list)\n",
    "    for title, similarity in zip (titles, similarities):\n",
    "        print(f\"{similarity:.2f} -- {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a89cfd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"Toy Story\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf54a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"Batman Returns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8931cd6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print_recommendations(rs_model, movies_mapper, \"Godfather, The\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
